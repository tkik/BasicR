---
title: "Statistics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# STATISTICAL TESTS

Statistical tests are used to test assertions about populations. The aim is to determine whether there is enough evidence to _reject_ a null hypothesis. This is shown by way of p-values. 

[How it works for continuous data?](https://shiny.rit.albany.edu/stat/sampdist/interactive%20explanation)

[How to interpret p-value?](https://rpsychologist.com/pvalue/)

For normal continuous data with one or two groups of interest, the t-test (paired or unpaired) is used to compare means. For non-normal data, non-parametric tests such as the Wilcoxon Test is used that compares ranks. For  three or more groups, the ANOVA method (for normal data) compares the means with the help of variance calculations between groups and within groups. and Kruskal-Wallis test (for non-normal data) to compare ranks for non-normal data for calculations between groups and within groups.


```{r}
library(tidyverse)
library(openintro)
```



```{r}
hip<-read.csv("../inst/extdata/hip_data.csv")
summary(hip)
```


For categorical data, the chi-squared test shows if the observed values in each sub-category differ to the expected values by chance or not. 

## Two-sample T-test or Wilcoxon-rank tests 

T-tests or Wilcoxon-rank tests can be used to describe the association between two continuous variables.

### Unpaired tests

Explore the distributions of ages in men and women

```{r}
hip %>% filter(!is.na(sex)) %>% ggplot(aes(sex,age)) + geom_boxplot()

hip %>% filter(!is.na(sex)) %>% ggplot(aes(sex,height)) + geom_boxplot()

hip %>% 
  select(height, weight, sex) %>% 
   filter(!is.na(sex)) %>% 
  pivot_longer(cols = c(height, weight)) %>%
  ggplot()+geom_boxplot(aes(sex, log10(value), fill=name))

```

An unpaired t-test looks at whether the means of continuous variables differs between two groups.  For example we could see whether there is an association between the average age of men and the average age of women. You can get the means with `estimate` and the p-value with `p.value` using the accessor `$`.


```{r}

library(broom)
t<-t.test(age~sex,data=hip %>% filter(!is.na(sex)))
t
tidy(t)

t$estimate
t$p.value
```

This indicates that there is a difference in the mean age of men and women of `r t$estimate[2]-t$estimate[1]`, and this difference is significant with a p.value of `r round(t$p.value,8)`



The `p-value` from the t-test can be added to the boxplots using `stat_compare_means` from the library `ggpubr`.

```{r}
library(ggpubr)
hip %>% filter(!is.na(sex)) %>% ggplot(aes(sex,age)) + geom_boxplot() + stat_compare_means(method = "t.test")
```

However, check the distribution of age in men and women separately using the _histogram_ and _qq-plot_. It does not look normally distributed.

```{r}
hip %>% filter(!is.na(sex)) %>% ggplot(aes(age, fill=sex)) + facet_grid(~sex) + geom_histogram()
hip %>% filter(!is.na(sex)) %>% ggplot(aes(sample=scale(age), fill=sex)) + facet_grid(~sex) + geom_qq() + geom_abline() 
```



Instead of the t-test, one should use the Wilcoxon rank-sum test.

```{r}
w<-wilcox.test(age~sex,data=hip %>% filter(!is.na(sex)))
w

est<-hip %>% filter(!is.na(sex)) %>% group_by(sex) %>% summarize(median=median(age,na.rm=TRUE))
est
```

This indicates that there is a difference in the median age of men and women of `r est[2,2]-est[1,2]`, and this difference is significant with a p.value of `r round(w$p.value,8)`


Similarly, add the `p-value`  to the boxplots using `stat_compare_means` from the library `ggpubr`. The Wilcox-test is the default method, so we do not need to add an argument to the funciton.

```{r}
hip %>% filter(!is.na(sex)) %>% ggplot(aes(sex,age)) + geom_boxplot() + stat_compare_means()
```

### Paired tests

The paired t-test would be used to compare the difference in repeated measures on the same individuals. For example, in the dataset,  Performance at baseline and 6-months are given by `ohs0` and `ohs6`. Use a paired t-test to analyse the difference in the distributions. Note the extra `paired` argument in the `t.test` function. Use the using the accessor `$` or `pull` to extract the relevant variables.


```{r}

t<-t.test(hip$ohs0,hip$ohs6, paired=TRUE)
t

t$estimate
t$p.value
```

This indicates that there is a difference in the mean performance between baseline and 6 months of `r t$estimate`, and this difference is highly significant.

Calculate the difference between the performances at the 2 time points and use a histogram to represent the distribution. Use `geom_vline` to draw a dotted line at 0 to represent the _Null distribution_.


```{r}
hip %>% mutate(diff=ohs6-ohs0) %>% ggplot(aes(diff)) + geom_histogram() +  geom_vline(xintercept = 0, lty=2) 
```

The non-parametric version is the Wilcoxon signed-rank test, and this is also significant.

```{r}
w<-wilcox.test(hip$ohs0,hip$ohs6,paired=TRUE)
w

w$p.value


```



## ANOVA (Analysis of Variance)

A one-way ANOVA can be used when there are more than two groups.  

For example split the variable of satisfaction into 3 groups. 

```{r}
hip<-hip %>% mutate(satisfaction.cat=factor(ifelse(satisfaction<80,"Low",ifelse(between(satisfaction,80,90),"Medium","High")), levels=c("Low","Medium","High")))

hip %>% pull(satisfaction.cat) %>% table()


```

Run an ANOVA to test the relationship between age and satisfaction.cat

```{r}
a<-aov(age~satisfaction.cat,data=hip)

s<-summary(a)

s
```


This suggests that there is very little evidence that satisfaction groups are different with respect to age. with a p-value of `r round(s[[1]][1,5],4)`

## Chi-squared test

To look at the association between two categorical variables we can perform a chi squared test Study the relationshipo between sex and satisfaction.cat using a  contingency table and a bar plot.


```{r}

hip %>% filter(!is.na(satisfaction.cat))  %>% filter(!is.na(sex)) %>% ggplot(aes(sex,fill=satisfaction.cat)) + geom_bar(position="dodge")

t <- hip %>% select(sex,satisfaction.cat) %>% table()
t
```


Use `prop.table` and  `geom_bar(stat="identity")` to report the relative frequencies of satisfaction between the sexes.

```{r}
p<-prop.table(t, margin =1)

p %>% as.data.frame() %>% ggplot(aes(x=sex,y=Freq,fill=satisfaction.cat)) + geom_bar(stat="identity")

```

You can get relative risk between the sexes of being satisfied at a specific level from the following command.

```{r}
p[1,]/p[2,]
```


Test this relationship using the `chi.test` command on the contingency table.

```{r}
c<-chisq.test(t)
c

```

The relative risks are close to 1. Alongwith the test, this indicates that there is very little evidence of a relationship between satisfaction.cat and sex, with a p.value of `r round(c$p.value,4)`.

## Exercise

1. Check whether there is a difference in the performance at baseline `ohs0` between the males and females. Study the distributions and decide which test to use. 

2. We used an ANOVA to study the distribution of satisfaction.cat and age. Check the distribution of age over the 3 satisfaction.cat groups. Does it pass normality assumptions? If you are unsure, use Kruskal-Wallis test. Hint you can get information using `?kruskal.test`.

3. Use ANOVA or Kruskal-Wallis test to check if there is an association between baseline ohs0 and satisfaction.cat groups.

4. Create a table of retired against satisfaction groups. Does the chi-square test show an association between the two? 


# Linear associations

```{r}

hip %>% 
  ggplot()+geom_point(aes(age, weight))


hip %>%
  filter(!is.na(weight)) %>% 
  summarise(cor(weight, age))


```

Linear regression 

```{r}

m1 <- lm(weight ~ age, data = hip)

```

The first argument in the function `lm()` is a formula that takes the form `y ~ x`.

The output of `lm()` is an object that contains all of the information we need about the linear model that was just fit.
We can access this information using the `tidy()` function.

```{r summary-m1}
tidy(m1)
summary(m1)
```

```{r}
glance(m1)
```


For this model, 7.7% of the variability in `weight` is explained by `age`.


<span style="font-size:4em;">Correlation is not causation!!! </span> 


```{r, echo = FALSE, out.width = "70%", eval=TRUE}
knitr::include_graphics("https://images.squarespace-cdn.com/content/v1/5bfc8dbab40b9d7dd9054f41/1635480696871-Y3HAE2GV9EUJB5P7UANT/FCUQhjvVUAUAPlh.jpg?format=750w?raw=true", dpi = 100)
```


## Prediction and prediction errors

Let's create a scatterplot with the least squares line for `m1` laid on top.

```{r reg-with-line}
ggplot(data = hip, aes(x = age, y = weight, color=sex)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```


Here, we are literally adding a layer on top of our plot.
`geom_smooth` creates the line by fitting a linear model.
It can also show us the standard error `se` associated with our line, but we'll suppress that for now.

This line can be used to predict $y$ at any value of $x$.
When predictions are made for values of $x$ that are beyond the range of the observed data, it is referred to as *extrapolation* and is not usually recommended.
However, predictions made within the range of the data are more reliable.
They're also used to compute the residuals.



## Model diagnostics

To assess whether the linear model is reliable, we need to check for (1) linearity, (2) nearly normal residuals, and (3) constant variability.

In order to do these checks we need access to the fitted (predicted) values and the residuals.
We can use the `augment()` function to calculate these.

```{r}
m1_aug <- augment(m1)
```



###Linearity###

You already checked if the relationship between `pf_score` and `pf_expression_control` is linear using a scatterplot.
We should also verify this condition with a plot of the residuals vs. fitted (predicted) values.

```{r residuals}
ggplot(data = m1_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("Residuals")
```



Notice here that `m1` can also serve as a data set because stored within it are the fitted values ($\hat{y}$) and the residuals.
Also note that we're getting fancy with the code here.
After creating the scatterplot on the first layer (first line of code), we overlay a red horizontal dashed line at $y = 0$ (to help us check whether the residuals are distributed around 0), and we also rename the axis labels to be more informative.

Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?


###Nearly normal residuals###

To check this condition, we can look at a histogram of the residuals.

```{r hist-res}
ggplot(data = m1_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 0.25) +
  xlab("Residuals")
```

Based on the histogram, does the nearly normal residuals condition appear to be violated? Why or why not?



###Constant variability###


```{r var-res}
ggplot(data = m1_aug, aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  xlab("Fitted values") +
  ylab("SD Residuals")
```

Based on the residuals vs. fitted plot, does the constant variability condition appear to be violated? Why or why not?




## EXERCISES


1\. Fit a new model that uses  `pf_expression_control` to predict `hf_score`, or the total human freedom score. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between human freedom and the amount of political pressure on media content?

2\. If someone saw the least squares regression line and not the actual data, how would they predict a country's `hf_score` for one with a 3 rating for `pf_expression_control`? Is this an overestimate or an underestimate, and by how much? In other words, what is the residual for this prediction?

3\. How does this relationship compare to the relationship between `pf_score` and `pf_expression_control`?
    Use the $R^2$ values from the two model summaries to compare.
    Does your `pf_expression_control` seem to predict `hf_score` better?
    Why or why not?

4\. Check the model diagnostics using appropriate visualisations and evaluate if the model conditions have been met.

5\. Pick another pair of variables of interest and visualise the relationship between them.
    Do you find the relationship surprising or is it what you expected.
    Discuss why you were interested in these variables and why you were/were not surprised by the relationship you observed.
